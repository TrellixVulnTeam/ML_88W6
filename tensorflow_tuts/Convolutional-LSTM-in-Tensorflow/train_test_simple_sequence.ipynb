{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "import layer_def as ld\n",
    "import BasicConvLSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dim = (32,32) \n",
    "image_num = 3200\n",
    "batch_size_ = 16\n",
    "seq_len_ = 4\n",
    "seq_start_ = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', './checkpoints/train_store_conv_lstm',\n",
    "                            \"\"\"dir to store trained net\"\"\")\n",
    "tf.app.flags.DEFINE_integer('seq_length', 4,\n",
    "                            \"\"\"size of hidden layer\"\"\")\n",
    "tf.app.flags.DEFINE_integer('seq_start', 2,\n",
    "                            \"\"\" start of seq generation\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_step', 200000,\n",
    "                            \"\"\"max num of steps\"\"\")\n",
    "tf.app.flags.DEFINE_float('keep_prob', .8,\n",
    "                            \"\"\"for dropout\"\"\")\n",
    "tf.app.flags.DEFINE_float('lr', .1,\n",
    "                            \"\"\"for dropout\"\"\")\n",
    "tf.app.flags.DEFINE_integer('batch_size', 16,\n",
    "                            \"\"\"batch size for training\"\"\")\n",
    "tf.app.flags.DEFINE_float('weight_init', .1,\n",
    "                            \"\"\"weight init for fully connected layers\"\"\")\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_seq_len(data, seq_len = seq_len_):\n",
    "    reshaped_x = []\n",
    "    for x in data:\n",
    "        temp = x.reshape([-1,seq_len,x.shape[1],x.shape[2]])\n",
    "        reshaped_x.append(temp)\n",
    "    return reshaped_x\n",
    "\n",
    "\n",
    "def gen_batch(data, batch_size = batch_size_):\n",
    "    X = data[0]\n",
    "    Y = data[1]\n",
    "    for i in range(0 , len(X), batch_size):\n",
    "        yield X[i: i + batch_size], Y[i: i + batch_size]\n",
    "        \n",
    "        \n",
    "def my_shuffle(data):\n",
    "    \n",
    "    suffled_x = []\n",
    "    start_state = random.getstate()\n",
    "    for x in data:\n",
    "        random.shuffle(x)\n",
    "        random.setstate(start_state)\n",
    "        suffled_x.append(x)        \n",
    "    return suffled_x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X = np.asarray([np.ones(img_dim) for n in range(image_num)])\n",
    "raw_Y = np.cumsum(raw_X,axis=0)\n",
    "\n",
    "data = (raw_X, raw_X)\n",
    "reshaped_data = reshape_seq_len(data, seq_len_)\n",
    "shuffled_data = my_shuffle(reshaped_data)\n",
    "data_generator = gen_batch(shuffled_data, batch_size_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(inputs, hidden, lstm=True):\n",
    "    \n",
    "    y_0 = inputs\n",
    "    \n",
    "    if lstm:\n",
    "    # conv lstm cell \n",
    "        with tf.variable_scope('conv_lstm', initializer = tf.random_uniform_initializer(-.01, 0.1), reuse=tf.AUTO_REUSE):\n",
    "                cell = BasicConvLSTMCell.BasicConvLSTMCell([32,32], [3,3], 1)\n",
    "                if hidden is None:\n",
    "                    hidden = cell.zero_state(FLAGS.batch_size, tf.float32) \n",
    "                y_1, hidden = cell(y_0, hidden)\n",
    "    else:\n",
    "        y_1 = ld.conv_layer(y_0, 3, 1, 8, \"encode_3\")\n",
    "\n",
    "    return y_1, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  \"\"\"Train ring_net for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    # make inputs\n",
    "    x = tf.placeholder(tf.float32, [None, seq_len_, 32, 32, 1])\n",
    "\n",
    "    # possible dropout inside\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    x_dropout = tf.nn.dropout(x, keep_prob)\n",
    "\n",
    "    # create network\n",
    "    x_unwrap = []\n",
    "\n",
    "    # conv network\n",
    "    hidden = None\n",
    "    for i in range(seq_len_-1):\n",
    "      if i < seq_start_:\n",
    "        x_1, hidden = network_template(x_dropout[:,i,:,:,:], hidden)\n",
    "      else:\n",
    "        x_1, hidden = network_template(x_1, hidden)\n",
    "      x_unwrap.append(x_1)\n",
    "\n",
    "    # pack them all together \n",
    "    x_unwrap = tf.stack(x_unwrap)\n",
    "    x_unwrap = tf.transpose(x_unwrap, [1,0,2,3,4])\n",
    "\n",
    "    # this part will be used for generating video\n",
    "    x_unwrap_g = []\n",
    "    hidden_g = None\n",
    "    for i in range(50):\n",
    "      if i < seq_start_:\n",
    "        x_1_g, hidden_g = network_template(x_dropout[:,i,:,:,:], hidden_g)\n",
    "      else:\n",
    "        x_1_g, hidden_g = network_template(x_1_g, hidden_g)\n",
    "      x_unwrap_g.append(x_1_g)\n",
    "\n",
    "    # pack them generated ones\n",
    "    x_unwrap_g = tf.stack(x_unwrap_g)\n",
    "    x_unwrap_g = tf.transpose(x_unwrap_g, [1,0,2,3,4])\n",
    "\n",
    "    # calc total loss (compare x_t to x_t+1)\n",
    "    loss = tf.nn.l2_loss(x[:,seq_start_+1:,:,:,:] - x_unwrap[:,seq_start_:,:,:,:])\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # training\n",
    "    train_op = tf.train.AdamOptimizer(FLAGS.lr).minimize(loss)\n",
    "    \n",
    "    # List of all Variables\n",
    "    variables = tf.global_variables()\n",
    "\n",
    "    # Build a saver\n",
    "    saver = tf.train.Saver(tf.global_variables())   \n",
    "\n",
    "    # Summary op\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start running operations on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    \n",
    "    print(\"init network from scratch\")\n",
    "    sess.run(init)\n",
    "\n",
    "    # Summary op\n",
    "    graph_def = sess.graph.as_graph_def(add_shapes=True)\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.train_dir, graph_def=graph_def)\n",
    "\n",
    "    for step in range(FLAGS.max_step):\n",
    "      dat = data_generator.__next__()\n",
    "      dat = np.expand_dims(dat[1], axis= 5)\n",
    "      t = time.time()\n",
    "      _, loss_r = sess.run([train_op, loss],feed_dict={x:dat, keep_prob:FLAGS.keep_prob})\n",
    "      elapsed = time.time() - t\n",
    "\n",
    "      if step%1 == 0 and step != 0:\n",
    "        summary_str = sess.run(summary_op, feed_dict={x:dat, keep_prob:FLAGS.keep_prob})\n",
    "        summary_writer.add_summary(summary_str, step) \n",
    "        print(\"time per batch is \" + str(elapsed))\n",
    "        print(step)\n",
    "        print(loss_r)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_template = tf.make_template('network', network)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
