{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import functional_ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_target_generator(min_duration=5, max_duration=50):\n",
    "\n",
    "    while True:\n",
    "        duration = np.random.randint(min_duration, max_duration)\n",
    "        inputs = np.random.randn(duration).astype(np.float32)\n",
    "        targets = np.cumsum(inputs).astype(np.float32)\n",
    "        yield inputs.reshape(-1, 1), targets.reshape(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class school(object):\n",
    "    \n",
    "    def __init__(self, first, last):\n",
    "        \n",
    "        self.first = first\n",
    "        self.last = last\n",
    "        self._full = self._get_full()\n",
    "        \n",
    "    def _get_full(self):\n",
    "        full_name = \"{} {}\".format(self.last, self.first)\n",
    "        return full_name\n",
    "         \n",
    "    @property\n",
    "    def full(self):        \n",
    "        return self._full\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chris Jacob\n",
      "chris Jacob\n"
     ]
    }
   ],
   "source": [
    "std_1 = school(\"Jacob\", \"chris\")\n",
    "print(std_1.full)\n",
    "std_1.first = \"jeon\"\n",
    "print(std_1.full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, hidden_layer_size, input_size, target_size, init_scale=0.1):\n",
    "        \n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.input_size = input_size\n",
    "        self.target_size = target_size\n",
    "        self.init_scale = init_scale\n",
    "        \n",
    "        self._inputs = tf.placeholder(tf.float32, shape=[None, input_size],\n",
    "                                      name='inputs')\n",
    "        self._targets = tf.placeholder(tf.float32, shape=[None, target_size],\n",
    "                                       name='targets')\n",
    "        \n",
    "        initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "        with tf.variable_scope('model', initializer=initializer):\n",
    "            self._states, self._predictions = self._compute_predictions()\n",
    "            self._loss = self._compute_loss()\n",
    "    \n",
    "    def _vanilla_rnn_step(self, h_prev, x):\n",
    "        \"\"\" Vanilla RNN step.bb\n",
    "\n",
    "        Args:\n",
    "            h_prev: A 1-D float32 Tensor with shape `[hidden_layer_size]`.\n",
    "            x: A 1-D float32 Tensor with shape `[input_size]`.\n",
    "\n",
    "        Returns:\n",
    "            The updated state `h`, with the same shape as `h_prev`.\n",
    "        \"\"\"\n",
    "\n",
    "        h_prev = tf.reshape(h_prev, [1, self.hidden_layer_size])\n",
    "        x = tf.reshape(x, [1, self.input_size])\n",
    "\n",
    "        with tf.variable_scope('rnn_block'):\n",
    "            W_h = tf.get_variable(\n",
    "                'W_h', shape=[self.hidden_layer_size, self.hidden_layer_size])\n",
    "            W_x = tf.get_variable(\n",
    "                'W_x', shape=[self.input_size, self.hidden_layer_size])\n",
    "            b = tf.get_variable('b', shape=[self.hidden_layer_size],\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "            h = tf.tanh( tf.matmul(h_prev, W_h) + tf.matmul(x, W_x) + b )\n",
    "            h = tf.reshape(h, [self.hidden_layer_size], name='h')\n",
    "            \n",
    "        return h\n",
    "\n",
    "    def _compute_predictions(self):\n",
    "        \"\"\" Compute vanilla-RNN states and predictions. \"\"\"\n",
    "\n",
    "        with tf.variable_scope('states'):\n",
    "            initial_state = tf.zeros([self.hidden_layer_size],\n",
    "                                     name='initial_state')\n",
    "            states = tf.scan(self._vanilla_rnn_step, self.inputs,\n",
    "                                         initializer=initial_state, name='states')\n",
    "\n",
    "        with tf.variable_scope('predictions'):\n",
    "            W_pred = tf.get_variable(\n",
    "                'W_pred', shape=[self.hidden_layer_size, self.target_size])\n",
    "            b_pred = tf.get_variable('b_pred', shape=[self.target_size],\n",
    "                                     initializer=tf.constant_initializer(0.0))\n",
    "            predictions = tf.add(tf.matmul(states, W_pred), b_pred, name='predictions')\n",
    "            \n",
    "        return states, predictions\n",
    "\n",
    "    def _compute_loss(self):\n",
    "        \"\"\" Compute l2 loss between targets and predictions. \"\"\"\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "            loss = tf.reduce_mean((self.targets - self.predictions)**2, name='loss')\n",
    "            return loss\n",
    "    \n",
    "    @property\n",
    "    def inputs(self):\n",
    "        \"\"\" A 2-D float32 placeholder with shape `[dynamic_duration, input_size]`. \"\"\"\n",
    "        return self._inputs\n",
    "    \n",
    "    @property\n",
    "    def targets(self):\n",
    "        \"\"\" A 2-D float32 placeholder with shape `[dynamic_duration, target_size]`. \"\"\"\n",
    "        return self._targets\n",
    "    \n",
    "    @property\n",
    "    def states(self):\n",
    "        \"\"\" A 2-D float32 Tensor with shape `[dynamic_duration, hidden_layer_size]`. \"\"\"\n",
    "        return self._states\n",
    "    \n",
    "    @property\n",
    "    def predictions(self):\n",
    "        \"\"\" A 2-D float32 Tensor with shape `[dynamic_duration, target_size]`. \"\"\"\n",
    "        return self._predictions\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        \"\"\" A 0-D float32 Tensor. \"\"\"\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    \n",
    "    def __init__(self, loss, initial_learning_rate, num_steps_per_decay,\n",
    "                 decay_rate, max_global_norm=1.0):\n",
    "        \"\"\" Create a simple optimizer.\n",
    "        \n",
    "        This optimizer clips gradients and uses vanilla stochastic gradient\n",
    "        descent with a learning rate that decays exponentially.\n",
    "        \n",
    "        Args:\n",
    "            loss: A 0-D float32 Tensor.\n",
    "            initial_learning_rate: A float.\n",
    "            num_steps_per_decay: An integer.\n",
    "            decay_rate: A float. The factor applied to the learning rate\n",
    "                every `num_steps_per_decay` steps.\n",
    "            max_global_norm: A float. If the global gradient norm is less than\n",
    "                this, do nothing. Otherwise, rescale all gradients so that\n",
    "                the global norm because `max_global_norm`.\n",
    "        \"\"\"\n",
    "        \n",
    "        trainables = tf.trainable_variables()\n",
    "        grads = tf.gradients(loss, trainables)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, clip_norm=max_global_norm)\n",
    "        grad_var_pairs = zip(grads, trainables)\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False, dtype=tf.int32)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            initial_learning_rate, global_step, num_steps_per_decay,\n",
    "            decay_rate, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        self._optimize_op = optimizer.apply_gradients(grad_var_pairs,\n",
    "                                                      global_step=global_step)\n",
    "    \n",
    "    @property\n",
    "    def optimize_op(self):\n",
    "        \"\"\" An Operation that takes one optimization step. \"\"\"\n",
    "        return self._optimize_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, model, optimizer, generator, num_optimization_steps,\n",
    "          logdir='./logdir'):\n",
    "\n",
    "    \"\"\" Train.\n",
    "    \n",
    "    Args:\n",
    "        sess: A Session.\n",
    "        model: A Model.\n",
    "        optimizer: An Optimizer.\n",
    "        generator: A generator that yields `(inputs, targets)` tuples, with\n",
    "            `inputs` and `targets` both having shape `[dynamic_duration, 1]`.\n",
    "        num_optimization_steps: An integer.\n",
    "        logdir: A string. The log directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.exists(logdir):\n",
    "        shutil.rmtree(logdir)\n",
    "        \n",
    "    tf.summary.scalar('loss', model.loss)\n",
    "    \n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "    update_loss_ema = ema.apply([model.loss])\n",
    "    loss_ema = ema.average(model.loss)\n",
    "    tf.summary.scalar('loss_ema', loss_ema)\n",
    "        \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter(logdir=logdir, graph=sess.graph)\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for step in range(num_optimization_steps):\n",
    "        inputs, targets = generator.__next__()\n",
    "        loss_ema_, summary, _, _ = sess.run(\n",
    "            [loss_ema, summary_op, optimizer.optimize_op, update_loss_ema],\n",
    "            {model.inputs: inputs, model.targets: targets})\n",
    "        summary_writer.add_summary(summary, global_step=step)\n",
    "        print('\\rStep %d. Loss EMA: %.6f.' % (step+1, loss_ema_), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "generator = input_target_generator()\n",
    "model = Model(hidden_layer_size=256, input_size=1, target_size=1, init_scale=0.1)\n",
    "optimizer = Optimizer(model.loss, initial_learning_rate=1e-2, num_steps_per_decay=15000,\n",
    "                      decay_rate=0.1, max_global_norm=1.0)\n",
    "\n",
    "sess = tf.Session()\n",
    "train(sess, model, optimizer, generator, num_optimization_steps=45000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
